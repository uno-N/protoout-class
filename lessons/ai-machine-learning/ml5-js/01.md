# ml5.jsでさまざまなAIに触れてみる

![](https://i.gyazo.com/da0b3decdfb5c8169f6238d07568967b.png)

## 1. [ml5.js](https://ml5js.org)とは

<a href="https://gyazo.com/384a0db0a116ef373b649f0f637c2cfd"><img src="https://i.gyazo.com/384a0db0a116ef373b649f0f637c2cfd.png" alt="Image from Gyazo" width="700"/></a>

**Webアプリケーション上で機械学習を手軽に行うことができる** JSライブラリです。

- [ml5js - もっと簡単にWeb上で機械学習を MOONGIFT](https://www.moongift.jp/2018/07/ml5js-%E3%82%82%E3%81%A3%E3%81%A8%E7%B0%A1%E5%8D%98%E3%81%ABweb%E4%B8%8A%E3%81%A7%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%92/)

<details>
  <summary>ml5.jsの歴史（興味のある方のみどうぞ）</summary>

1. Web上（フロントエンド）での機械学習といえば、[Tensorflow.js](https://www.tensorflow.org/js?hl=ja)が主流となっています。  
また、機械学習とは直接関係のない文脈で、「デジタルアートのためのアーティスト向け開発環境」として[Processing（P5）](https://processing.org)というものがあります。

2. このProcessingはPC上で独立して動作するアプリケーション（開発環境・IDE）ですが、これをWeb上（フロントエンド）で使えるようにということで、[p5.js](https://p5js.org)というものが新たに誕生しました。

3. さらに、このp5.jsで機械学習を手軽に使いたい！という需要から、Tensorflow.jsの一部を取り込んだp5.jsの拡張ライブラリとして[ml5.js](https://ml5js.org)が作成されました。

</details>

### 1-1. そもそもライブラリとは

Webアプリを作り込んでいると、複雑な処理を1つの関数もしくはファイルに綺麗にまとめて作ることがあります。  
そして大体こういうものは、他人にとっても便利なものだったりするわけです。  
そうやって公開されている、特定の処理を行ってくれる関数などが含まれたソースコードのことを「**ライブラリ**」といいます。

> JavaScriptのライブラリは、慣例的に「なんちゃら.js」といった名付けがされることが非常に多いです。  
> またライブラリはJSだけに限らず、汎用的なソースコードのセットという意味ではCSSもHTMLでも存在します。

注：CodePenでライブラリを使う場合は `Settings` から設定します。ここで紹介するml5.jsもそのようにして読み込みを行っています。

[![Image from Gyazo](https://i.gyazo.com/5611f560daa626402fc4a89d887ee9de.png)](https://gyazo.com/5611f560daa626402fc4a89d887ee9de)

「機械学習は学習によるモデル作成と推論の組み合わせ」と一番最初にお話ししましたが、 **ml5.jsはすでに学習が終わっているモデルをいくつも備えているので、好きなものを選んで推論を行うだけの機械学習ライブラリ** となっています。

注：一部の機械学習はモデルを自前で用意する必要があります。

### 1-2. どんな学習モデルがある？

授業を行っている2021年09月時点で、以下の各種機械学習を扱うことができます。  
機械学習は、そのモデルによって入力と出力の数や形式が決まっていますが、ここでは **これだけの数の汎用的なモデルにml5.jsが対応している** ということになります。  

これらの中には、 **モデルがすでにプリセットされていて自分で用意しなくてよい（比較的簡単）** ものから、自分でモデルのファイルをどこかからダウンロードしてくるか、自分で作成する（= データセットを用意して学習する）必要があるものまでさなざまな種類があります。

| モデル名 | 入力データ | 内容 | モデルのプリセット有 |
|:--|:--|:--|:--:|
| Image Classifier | 画像 | 映っている物体が何であるか分類する | ✔︎ |
| PoseNet | 画像 | 人の身体の姿勢状態（ボーン）を推定する | ✔︎ |
| BodyPix | 画像 | 画像中の人の身体とそうでない部分の領域を切り分ける | ✔︎ |
| UNET | 画像 | BodyPixと同じだが医学的観点でより精度を高くしたもの | ✔︎ |
| Handpose | 画像 | 画像内の手の範囲検出と関節位置の推定 | ✔︎ |
| Facemesh | 画像 | 顔面にある多数の解剖学的特徴点を検出 | ✔︎ |
| Face-Api | 画像 | 表情分類と顔面パーツ位置の取得 | ✔︎ |
| StyleTransfer | 画像 | 入力画像に別の画像の画風（特徴）を適用させる | |
| Pix2Pix | 画像 | ペア画像の関係性を学習し1枚の画像からその対になる画像を生成 | |
| CVAE | 画像 | 与えられた画像の特徴のみを学習しキーワードに対応する大雑把な画像を生成する | |
| DCGAN | 画像 | GANによる画像生成をさらに自然に行えるようにしたもの | |
| SketchRNN | 画像 | 手書きの線を速度や順序の時系列変化まで含めて認識し分類する | |
| ObjectDetector (YOLO) | 画像 | 映っている全物体の分類と位置を検出する | ✔︎ |
| SoundClassifier | 音声 (英語) | 18種類の簡単な英語の発音を認識する | ✔︎ |
| PitchDetection | 音声 | 入力された音声の周波数（主成分）を推定する | |
| CharRNN | 文字 (英語) | 既存文章の学習により同様の特徴を持った任意の長さの別の文章を生成する | |
| Sentiment | 文字 (英語) | 入力された文章がポジティブかネガティブかを数値で表現する | ✔︎ |
| Word2Vec | 文字 (英語) | 入力された単語を数値化し、複数の語を組み合わせたり距離（関係性）を測ったりする | |

画像（静止画）は、映像（動画）でも同じです。  
静止画を1枚だけ利用するか、連続的に利用するかの違いのみです。

> 一部モデルでRNNとあるものは時間変化を考慮するため、時系列に沿った前後のデータを取り込む必要がありますが、今回はそこまで扱わないので気にしなくてかまいません。

## 2. ml5.jsに触れてみよう
以下のWebページにml5.jsのチュートリアルとサンプルがあります。  
有名な（規模の大きな）ライブラリはこのように開発者用のドキュメントが充実しているものが多いです。

はじめかた：[ml5.js Getting Started](https://learn.ml5js.org#/)  
サンプル集：[ml5 examples index](https://examples.ml5js.org)  

次に、いくつかのサンプルをCodePenで触ってみましょう。
基本的には前パートで行った「3. カメラ利用の準備」の応用になっています。  

### 2-1. 物体認識
まずは機械学習の中でもポピュラーで応用の効きやすい物体認識のモデルを試してみます。  
以下のリンクからサンプルコードを起動してみましょう。

サンプル：[ObjectDetector | ml5.js](https://codepen.io/pen/?template=OJgjoPa)  
（参考）ドキュメント：[ObjectDetector](https://learn.ml5js.org/#/reference/object-detector)

ページが開いたら `スタート `を押してみましょう。カメラが起動してStatusが「Ready」になれば認識されたものがResultへ表示されるはずです。
（下の画像は私が認識されて「Person」と表示されていますね。）

[![Image from Gyazo](https://i.gyazo.com/7356de0bac6b9c021f33ecd7a08f17ec.png)](https://gyazo.com/7356de0bac6b9c021f33ecd7a08f17ec)

是非、周りにあるものが認識されるか試してみてください！！

<details>
  <summary>コード解説（興味のある方のみどうぞ）</summary>

  今回のサンプルはカメラ利用準備のテンプレから以下の部分が変わっています。

  ①HTML  
[![Image from Gyazo](https://i.gyazo.com/2ccfeff56275b35bf8f6308f76f73ead.png)](https://gyazo.com/2ccfeff56275b35bf8f6308f76f73ead)

認識結果を表示する部分が追加されています。

```HTML
<!-- モデル状態と結果を表示する部分（整えるためtableを使用） -->
<table>
  <tr>
    <th>Status:</th>
    <td id="status">Not started</td>
  </tr>
  <tr>
    <th>Result:</th>
    <td id="result"></td>
  </tr>
</table>
```

table要素を使用することでStatusとResultを綺麗に縦に配置できます。

②JavaScript  
[![Image from Gyazo](https://i.gyazo.com/3faff2c28576be9a08b3ce7fd1ea530c.png)](https://gyazo.com/3faff2c28576be9a08b3ce7fd1ea530c)

まず以下の部分で物体認識のライブラリを準備します。

```JS
 // ml5.js ObjectDetector
  document.getElementById("status").textContent = "Model loading...";
  const detector = ml5.objectDetector("cocossd", {}, () => {
    document.getElementById("status").textContent = "Ready";
  });
```

そして以下の部分で物体認識処理を1秒ごとに実行し続けます。

```JS
  // 繰り返し処理
  function loop() {
    // ここで推論
    detector.detect(video, async (err, results) => {
      console.log(results);
      // 複数結果出ることあるのでラベルだけ抜き出す
      const messageArray = results.map((result) => result.label);
      document.getElementById("result").textContent = messageArray.join(" | ");
      // 推論終了1秒後にloop自身を実行（ループになる）
      setTimeout(loop, 1000);
    });
  }

  // 最初の繰り返し処理を実行（以後ループ）
  loop();
```

物体を認識したら以下の記述で画面に出力されます。
（先ほどHTMLの部分で出てきたresultという名前の要素を書き換えています）

```JS
document.getElementById("result").textContent = messageArray.join(" | ");
```

</details>

### 2-2. 手のひら認識
続いて手のひらを認識するモデルを試してみましょう。  
以下のサンプルを開いてみてください。

サンプル：[Handpose | ml5.js](https://codepen.io/pen/?template=vYZJzLj)  
（参考）ドキュメント：[Handpose](https://learn.ml5js.org/#/reference/handpose)

ページが開いたら `スタート `を押してみましょう。カメラが起動してStatusが「Ready」になれば手のひらが認識されるはずです。  
手のひらを認識するとそのサイズがResultに表示されます。

[![Image from Gyazo](https://i.gyazo.com/56405047bc7a82528042e7e8c97d8923.png)](https://gyazo.com/56405047bc7a82528042e7e8c97d8923)

カメラに手のひらを近づけたり遠ざけたりするとResultの数字が変わると思うのでやってみてください！！

<details>
  <summary>コード解説（興味のある方のみどうぞ）</summary>

今回はJavaScriptのみ変更されています。  
[![Image from Gyazo](https://i.gyazo.com/927a894cdfaf132e939d45821093e2cb.png)](https://gyazo.com/927a894cdfaf132e939d45821093e2cb)

モデルの準備部分は物体認識とほとんど変わりません。  
逆に、認識処理の部分は大きく違います。
今回のHandposeは物体認識のように繰り返し処理を自分で書かなくても、自動で関数を実行してくれる機能があります。  
このようなモデルは、ドキュメントを見ると `.on()` という処理が使えると説明されています。

```js
// 自動で { } の中身を繰り返してくれる！
handpose.on("predict", (results) => {
  if (results.length != 0) {
    // 結果配列の長さが0でない = 中身が入っている（手が認識されている）
  } else {
    // 結果配列の長さが0 = 中身がない（手が認識されていない）
  }
});
```

</details>

### 2-3. 音声認識
最後は音声を認識するモデルです。
以下のリンクからサンプルコードを起動してみましょう。

サンプル：[SoundClassifier | ml5.js](https://codepen.io/pen/?template=eYREQxm)  
（参考）ドキュメント：[SoundClassification](https://learn.ml5js.org/#/reference/sound-classifier)

今回はカメラの映像ではなく音を認識します。
`スタート `を押してStatusが「Ready」になったらPCのマイクに向かって喋ってみましょう。

※このモデルが認識できる単語はドキュメントを見ると以下のように記述されています。
>At this moment, with the ml5.soundClassifier(), you can use your own custom pre-trained speech commands or use the the "SpeechCommands18w" which can recognize "the ten digits from "zero" to "nine", "up", "down", "left", "right", "go", "stop", "yes", "no", as well as the additional categories of "unknown word" and "background noise"."

以下の例では「99.97％の確率で”one"」だと認識しています。

[![Image from Gyazo](https://i.gyazo.com/39dfa8b13d428909883f8b04e8967030.png)](https://gyazo.com/39dfa8b13d428909883f8b04e8967030)

上記の18単語でどの程度認識されるのか試してみましょう（発音の練習になるかも？）

  <details>
    <summary>コード解説（興味のある方のみどうぞ）</summary>

①HTML  
[![Image from Gyazo](https://i.gyazo.com/0036f3cbabba12ecf3441fb465d66d9d.png)](https://gyazo.com/0036f3cbabba12ecf3441fb465d66d9d)

HTMLは今までとは違いスタートボタンと結果表示用のテーブルだけが配置されていますね。  
音声認識モデルでは映像を処理しないのでvideoタグは削除されています。

②JavaScript  
こちらも映像に関する処理がなくなるため、完全に新規のプログラムになっています。


```js
async function start() {
  // カメラからの映像ストリーム取得、はここでは必要ありません

  // 音声認識モデルの変数定義だけ先に
  let classifier;

  // 繰り返し処理も先に準備しておきます
  function loop() {
    // ここで推論
    classifier.classify(async (err, results) => {
      // 結果の中身が入っているときだけ以下を実行
      if (results) {
        console.log(results);
        // 確率をパーセントに変換
        const percent = Math.round(10000 * results[0].confidence) / 100;
        // 結果メッセージを作ります（ラベルとパーセンテージ）
        const message = results[0].label + " (" + percent + "%)";
        // HTMLに反映
        document.getElementById("result").textContent = message;
      }
      // 推論終了1秒後にloop自身を実行（ループになる）
      setTimeout(loop, 1000);
    });
  }

  // ml5.js SoundClassifier
  document.getElementById("status").textContent = "Model loading...";
  classifier = ml5.soundClassifier(
    "SpeechCommands18w", // ここにTeachable Machineのモデルを入れることもできます
    {
      // オプション（最も可能性の高いワードがこの確率以上のときに結果を更新出力する）
      probabilityThreshold: 0.8
    },
    () => {
      // モデル読み込み完了したらここが実行
      document.getElementById("status").textContent = "Ready";
      // 最初の繰り返し処理を実行（以後ループ）
      loop();
    }
  );
 }
```

今までのようにモデルの準備を最初にしておらず、以下の記述でモデルの設定をしています。

```js
  classifier = ml5.soundClassifier(
    "SpeechCommands18w", // ここにTeachable Machineのモデルを入れることもできます
```

コメントで記載していますが、この後に実施するTeachable Machineで作成したモデルを使用することもできるので、興味があれば是非挑戦してみてください。

</details>

## 3. やってみよう

![](../common/homework.png)

### 10分間 チャレンジ！

ml5.jsで特定の物体を認識するWebアプリを作ってみましょう。

<手順>
1. [先ほどのサンプル](https://codepen.io/pen/?template=OJgjoPa) を使って色々な物体を検出させ、どのような名称として分類されるのかを確認  
   [![Image from Gyazo](https://i.gyazo.com/6b8ee5cfaa835449e6639c43ddac6113.png)](https://gyazo.com/6b8ee5cfaa835449e6639c43ddac6113)
2. 配列ではなく特定の物体が検出されたときのみ、「分類名」（ラベル）だけが表示されるようにする  
   こちらの[CodePen](https://codepen.io/pen/?template=eYRGqGp)を開いてIf文を書き換えましょう。  
   書き換えるのは27行目になります。
  　　[![Image from Gyazo](https://i.gyazo.com/73663ea36caa2084531e6c1f026344d5.png)](https://gyazo.com/73663ea36caa2084531e6c1f026344d5)

3. さらに分類名に応じて表示を変えてみましょう  
   たとえば日本語名称に変えてみましょう。  
   書き換えるのは29行目になります。
   [![Image from Gyazo](https://i.gyazo.com/c25a4e3b7a2effb7a9463c42b311bbc5.png)](https://gyazo.com/c25a4e3b7a2effb7a9463c42b311bbc5)

> 作業タイム … 10分  
> Twitter投稿タイム … 3分  

- アプリが作れなくても「こんなものが認識されたよ」でOKです。
- できてもできてなくても、**スクリーンショット・動画・CodePenのURL**などをハッシュタグ`#protoout`とともにTwitterに投稿しましょう。
- どんな形でSNS投稿すると見る側に取って「より良いか」を考えながら投稿してみてください。  
  ※たとえば、自身の手元にある「何か」がないと動きが確認できないものはCodePenのURLだけを投稿してもなかなか伝わらないですよね。

---

## 目次へ

[目次](https://github.com/protoout/po-common/tree/main/lessons)に戻ります
